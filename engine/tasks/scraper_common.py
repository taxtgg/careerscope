from engine import app
from .. import search
from . import scrapers
import celery
import elasticsearch_dsl.connections


"""
Dummy tasks
"""
@app.task
def task_status():
    """
    A dummy task to use for status checks.
    :return: None.
    """
    pass


@app.task
def reverse(msg):
    """
    Background task to reverse the specified string.
    :param msg: string to reverse.
    :return: reversed string.
    """
    return str(msg)[::-1]


@app.task
def ping():
    """
    Test task.
    :return: pong
    """
    # time.sleep(60)
    return 'pong'


"""
Common scraper tasks
"""

@app.task
def persist_get_openings_result(self, result):
    """
    Persists job scraper output to appropriate index in ElasticSearch.
    :param result: list of dicts generated by scrapers containing open positions.
    :return: boolean value, true if all required keys are present in each dict.
    """

    # TODO: Wrap this with module defaults (add abstraction layer)
    # TODO: Must handle possible elasticsearch.exceptions.ConnectionError
    elasticsearch_dsl.connections.connections.create_connection(hosts=['localhost'])

    if len(result) > 0:
        index = search.ScraperOutputIndex(company = result[0]['company'], output = str(result))
        index.save()
        return result[0]['company']

    else:
        return ''

@app.task
def execute_and_persist_scraper_results(self):
    """
    Runs and persists results of all scrapers in scrapers module
    :return: boolean value, true if all required keys are present in each dict.
    """

    # TODO: This is ugly. Need to run all scrapers in module, verifying and persisting each individually (not chord)
    # One possible solution is to place this into scrapers.py
    return celery.group(celery.chain(scrapers.slack_get_openings(), persist_get_openings_result(), process_raw_results()),
                        celery.chain(scrapers.stripe_get_openings(), persist_get_openings_result(), process_raw_results()))


@app.task
def process_raw_results(self, company):
    """
    Retrieves raw scraper results and squashes the iterations, producing 4 lists: new, closed from new and closed
    from current positions. Closed from new, is the list of positions that have not been processed into JobPostIndex.
    Closed from current is the list of positions that are already in JobPostIndex.
    :return: None.
    """

    if company == '':
        return # Nothing to do

    # TODO: Wrap this with module defaults (add abstraction layer)
    # TODO: Must handle possible elasticsearch.exceptions.ConnectionError
    elasticsearch_dsl.connections.connections.create_connection(hosts=['localhost'])

    s = search.ScraperOutputIndex.search()
    s = s.query('match', company = company).sort('extracted')
    results = s.execute()

    if results.hits.total == 0:
        return # Nothing to do

    # Let's grab oldest result and see if it has been processed
    if results[0].iterative:
        # It has been processed, all positions have been promoted to JobPost index
        new_positions = set()
        current_positions = { search.Position(p) for p in eval(results[0].output) }
        if results.hits.total == 1:
            return # Nothing to do
    else:
        # It has not been processed. Assuming first run of the scraper
        new_positions = { search.Position(p) for p in eval(results[0].output) }
        for item in new_positions:
            item.update( {"posted_date": results[0].extracted })
        current_positions = set()

    closed_from_new_positions = set()
    closed_from_current_positions = set()

    for i in range(1, results.hits.total):
        current_result_set = { search.Position(p) for p in eval(results[i].output) }

        new_this_iteration = current_result_set.difference(new_positions) | current_result_set.difference(current_positions)
        for item in new_this_iteration:
            item.update( {"posted_date": results[i].extracted })

        closed_from_new_this_iteration = new_positions.difference(current_result_set)
        for item in closed_from_new_this_iteration:
            item.update( {"closed_date": results[i].extracted })

        closed_from_current_this_iteration = current_positions.difference(current_result_set)
        for item in closed_from_current_this_iteration:
            item.update( {"closed_date": results[i].extracted })

        new_positions |= new_this_iteration
        closed_from_new_positions |= closed_from_new_this_iteration
        closed_from_current_positions |= closed_from_current_this_iteration

    # Propagate to JobPostIndex
    for position in new_positions:
        index = search.JobPostIndex(status = ['OPEN', 'NEEDS_LOCATION', 'NEEDS_KEYWORDS'], open = True, **position.__dict__)
        index.save()

    for position in closed_from_new_positions:
        index = search.JobPostIndex(status = ['CLOSED', 'NEEDS_LOCATION', 'NEEDS_KEYWORDS'], open = False, **position.__dict__)
        index.save()

    for position in closed_from_current_positions:
        js = search.JobPostIndex.search()
        js = s.query('match', open = True, company = position.company, title = position.title, department = position.department, locations_text = position.locations_text)
        results = js.execute()

        if results.hits.total == 1:
            index = search.JobPostIndex.get(results.hits[0].meta.id)
            # TODO: Process status properly
            index.update(status = ['CLOSED'], open = False, closed_date = position.closed_date)
        elif results.hits.total == 0:
            # TODO: handle not found position to update
            pass
        else:
            # TODO: throw error, to many results
            pass

    # Delete processed scraper results from index, using cached search
    s.delete()

    iter_result = [ p.__dict__ for p in (new_positions | current_positions) ]
    index = search.ScraperOutputIndex(company = company, output = str(iter_result), iterative = True)
    index.save()

@app.task
def enrich_location(self, result):
    """
    Takes results of jobs scraper and converts locations from strings to GeoLocation object
    :param result: list of dicts generated by scrapers containing open positions.
    :return: list of dicts, with GeoLocation object locations.
    """

    # TODO: Implement function
    pass


@app.task
def enrich_keywords(self, result):
    """
    Background task to reverse the specified string.
    :param result: list of dicts generated by scrapers containing open positions.
    :return: boolean value, true if all required keys are present in each dict.
    """

    # TODO: Implement function
    pass
