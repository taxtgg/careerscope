from engine import celery

import requests
import lxml.html as lh

"""
Dummy tasks
"""
@celery.task
def task_status():
    """
    A dummy task to use for status checks.
    :return: None.
    """
    pass


@celery.task
def reverse(msg):
    """
    Background task to reverse the specified string.
    :param msg: string to reverse.
    :return: reversed string.
    """
    return str(msg)[::-1]


@celery.task
def ping():
    """
    Test task.
    :return: pong
    """
    # time.sleep(60)
    return 'pong'


"""
Common scraper tasks
"""
@celery.task
def validate_get_openings_result(self, result):
    """
    Validates results generated by scraper tasks. Current version simply checks that all required keys are present in each dict
    :param result: list of dicts generated by scrapers containing open positions.
    :return: boolean value, true if all required keys are present in each dict.
    """


    requiredFields = {'company', 'title', 'url', 'locations'}
    #optionalFields = {'department', 'description'}

    for r in result:
        if not requiredFields.issubset(set(result.keys())):
            return False

    return True


@celery.task
def enrichLocation(self, result):
    """
    Takes results of jobs scraper and converts locatons from strings to GeoLocation object
    :param result: list of dicts generated by scrapers containing open positions.
    :return: list of dicts, with GeoLocation object locations.
    """

    # TODO: Implement function
    pass

@celery.task
def enrichKeywords(self, result):
    """
    Background task to reverse the specified string.
    :param result: list of dicts generated by scrapers containing open positions.
    :return: boolean value, true if all required keys are present in each dict.
    """

    # TODO: Implement function
    pass


"""
Company specific scraper tasks

All scrapers return a list of dicts with the following attributes:
requiredFields = {'company', 'title', 'url', 'locations'}
optionalFields = {'department', 'description'} 
"""

@celery.task
def slack_get_openings(self):
    """
    Get the jobs from Slack careers page
    :return: list of dicts representing open positions
    """


    company = 'Slack'
    url = 'https://slack.com/careers'

    req = requests.get(url)
    doc = lh.fromstring(req.text)
    doc.make_links_absolute(url)

    result = [ {'company': company,
                'title':position.cssselect('td:nth-child(2)')[0].text.encode('utf-8'),
                'url':position.cssselect('td:nth-child(5)')[0].find('a').get('href'),
                'locations':[position.get('data-location')],
                'department':deptSection.find('thead').find('tr').find('th').find('span').text }
               for deptSection in doc.cssselect('#main > section.careers-table > div > div.shadow-table > div:nth-child(1)')[0].iter('table')
               for position in deptSection.find('tbody').iter('tr') ]

    for position in result:
        req = requests.get(position['url'])
        doc = lh.fromstring(req.text)
        position['description'] = doc.cssselect('#main > section:nth-child(2) > div > div > div')[0].text_content().encode('utf-8')

    return result


@celery.task
def get_openings(self):
    """
    Get the jobs from Stripe Careers page
    :return: list of dicts representing open positions
    """

    company = 'Stripe'
    url = 'https://stripe.com/jobs'

    req = requests.get(url)
    doc = lh.fromstring(req.text)
    doc.make_links_absolute(url)

    result = [ {'company': company,
                'title': position.find('a').find('h3').text.encode('utf-8'),
                'url': position.find('a').get('href'),
                'department': docSection.find('h2').find('div').find('a').text.encode('utf-8') }
               for docSection in doc.get_element_by_id('openings').iter('section')
               for position in docSection.iter('li') ]

    for position in result:
        positionHTML = lh.fromstring(requests.get(position['url']).text)
        positionDetail = positionHTML.get_element_by_id("main-content").find('article')
        position['locations'] = [node.text for node in positionDetail.find('h1').iter('span')]
        position['description'] = positionDetail.text_content().encode('utf-8')

    return result